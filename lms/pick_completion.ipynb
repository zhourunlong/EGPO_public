{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from judges.pair_judge import PairJudge\n",
    "from utils import (\n",
    "    DATA_ROOT,\n",
    "    load_model_tokenizer,\n",
    ")\n",
    "\n",
    "NUM_DATA = 100\n",
    "NUM_RESPONSES = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "NUM_TOP = 2\n",
    "\n",
    "PREF_MODEL = \"vectorzhou/gemma-2-2b-it-preference_dataset_mixture2_and_safe_pku-Preference\"\n",
    "\n",
    "BASE_NAMES = {\n",
    "    \"vectorzhou/gemma-2-2b-it-alpaca-cleaned-SFT-PKU-SafeRLHF-OnlineIPO1-lora-0227213453\": \"oipoone\",\n",
    "    \"vectorzhou/gemma-2-2b-it-alpaca-cleaned-SFT-PKU-SafeRLHF-OnlineIPO2-lora-0227214805\": \"oipotwo\",\n",
    "    \"vectorzhou/gemma-2-2b-it-alpaca-cleaned-SFT-PKU-SafeRLHF-NashMD-lora-0227215018\": \"nmd\",\n",
    "    \"vectorzhou/gemma-2-2b-it-alpaca-cleaned-SFT-PKU-SafeRLHF-NashMDPG-lora-0301154042\": \"nmdpg\",\n",
    "    \"dummy/new_MPO\": \"mpo\",\n",
    "}\n",
    "REF_MODEL = \"vectorzhou/gemma-2-2b-it-alpaca-cleaned-SFT\"\n",
    "\n",
    "MAIN_MODEL = \"vectorzhou/gemma-2-2b-it-alpaca-cleaned-SFT-PKU-SafeRLHF-Extragradient-lora-0224142549\"\n",
    "MAIN_SHORT = \"eg\"\n",
    "\n",
    "ALL_MODEL_DICT = BASE_NAMES.copy()\n",
    "ALL_MODEL_DICT.update({REF_MODEL: \"pitref\"})\n",
    "\n",
    "def get_pref(compare_results, model1, model2, judge_type):\n",
    "    model1 = model1.split(\"/\")[-1]\n",
    "    model2 = model2.split(\"/\")[-1]\n",
    "\n",
    "    try:\n",
    "        pair_results = compare_results[model1][model2]\n",
    "        flip = False\n",
    "    except:\n",
    "        pair_results = compare_results[model2][model1]\n",
    "        flip = True\n",
    "\n",
    "    avg = sum([sum([x[judge_type] for x in pair_results[k][\"results\"]]) for k in range(NUM_DATA)]) / (NUM_DATA * NUM_RESPONSES)\n",
    "    if flip:\n",
    "        avg = 1 - avg\n",
    "    \n",
    "    return avg\n",
    "\n",
    "def get_all_prefs(compare_results, model1, model2, judge_type):\n",
    "    model1 = model1.split(\"/\")[-1]\n",
    "    model2 = model2.split(\"/\")[-1]\n",
    "\n",
    "    try:\n",
    "        pair_results = compare_results[model1][model2].copy()\n",
    "        flip = False\n",
    "    except:\n",
    "        pair_results = compare_results[model2][model1].copy()\n",
    "        flip = True\n",
    "    \n",
    "    pair_results = [[y[judge_type] for y in x[\"results\"]] for x in pair_results]\n",
    "    if flip:\n",
    "        pair_results = [1 - x for x in pair_results]\n",
    "    \n",
    "    return pair_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(\"eval_results\", \"compare.json\")\n",
    "with open(fn) as f:\n",
    "    compare_results = json.load(f)\n",
    "\n",
    "all_models = {}\n",
    "\n",
    "for base_name in list(BASE_NAMES.keys()) + [MAIN_MODEL]:\n",
    "    win_rates = {}\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model1 = f\"{base_name}-epoch-{epoch + 1}\"\n",
    "        win_rates[epoch] = get_pref(compare_results, model1, REF_MODEL, \"pref_model\")\n",
    "    sorted_win_rates = sorted(win_rates.items(), key=lambda x: x[1], reverse=True)\n",
    "    picked_ckpts = [sorted_win_rates[k][0] for k in range(NUM_TOP)]\n",
    "    picked_ckpts.sort()\n",
    "\n",
    "    all_models[base_name] = picked_ckpts\n",
    "\n",
    "for k, v in all_models.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_list = [REF_MODEL] + list(BASE_NAMES.keys())\n",
    "\n",
    "all_prefs = {}\n",
    "for epoch in all_models[MAIN_MODEL]:\n",
    "    all_prefs[epoch] = [[{m:{\"pref\": -1, \"epoch\": -1} for m in model2_list} for j in range(NUM_RESPONSES)] for i in range(NUM_DATA)]\n",
    "    for model2 in model2_list:\n",
    "        if model2 == REF_MODEL:\n",
    "            epochs = [None]\n",
    "        else:\n",
    "            epochs = all_models[model2]\n",
    "\n",
    "        for epoch2 in epochs:\n",
    "            model2_name = f\"{model2}-epoch-{epoch2 + 1}\" if epoch2 is not None else model2\n",
    "\n",
    "            prefs = get_all_prefs(compare_results, f\"{MAIN_MODEL}-epoch-{epoch + 1}\", model2_name, \"pref_model\")\n",
    "\n",
    "            for i in range(NUM_DATA):\n",
    "                for j in range(NUM_RESPONSES):\n",
    "                    if prefs[i][j] > all_prefs[epoch][i][j][model2][\"pref\"]:\n",
    "                        all_prefs[epoch][i][j][model2] = {\"pref\": prefs[i][j], \"epoch\": epoch2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "\n",
    "for model in list(all_models.keys()) + [REF_MODEL]:\n",
    "    if model == REF_MODEL:\n",
    "        epochs = [None]\n",
    "    else:\n",
    "        epochs = all_models[model]\n",
    "\n",
    "    for epoch in epochs:\n",
    "        model_name = f\"{model}-epoch-{epoch+1}\" if epoch is not None else model\n",
    "\n",
    "        _model_name = model_name.split(\"/\")[-1]\n",
    "        response_path = os.path.join(\"eval_results\", \"generation\", f\"{_model_name}.json\")\n",
    "        with open(response_path) as f:\n",
    "            responses[model_name] = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(text, maxline=10, maxchar=500):\n",
    "    lines = text.split(\"\\n\")\n",
    "    if len(lines) > maxline:\n",
    "        lines = lines[:maxline]\n",
    "    text = \"\\n\".join(lines)\n",
    "\n",
    "    if len(text) > maxchar:\n",
    "        text = text[:maxchar] + \"\\n...\"\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data = []\n",
    "\n",
    "for _ in range(5):\n",
    "    for epoch in all_models[MAIN_MODEL]:\n",
    "        max_pref, max_i, max_j = -1, -1, -1\n",
    "        for i in range(NUM_DATA):\n",
    "            for j in range(NUM_RESPONSES):\n",
    "                avg_pref = sum([all_prefs[epoch][i][j][m][\"pref\"] for m in model2_list]) / len(model2_list)\n",
    "                if avg_pref > max_pref and i not in used_data:\n",
    "                    max_pref = avg_pref\n",
    "                    max_i = i\n",
    "                    max_j = j\n",
    "        \n",
    "        # print(f\"{MAIN_MODEL}-epoch-{epoch+1}: Data #{max_i}, Response #{max_j}\", max_pref)\n",
    "\n",
    "        \n",
    "        main_model = f\"{MAIN_MODEL}-epoch-{epoch+1}\"\n",
    "\n",
    "        print(f\"\\\\begin{{filecontents*}}{{prompt{epoch}}}\")\n",
    "        print(responses[main_model][max_i]['prompt'])\n",
    "        print(\"\\end{filecontents*}\")\n",
    "        print(\"\")\n",
    "\n",
    "        for model2 in model2_list:\n",
    "            epoch2 = all_prefs[epoch][max_i][max_j][model2][\"epoch\"]\n",
    "            model_name = f\"{model2}-epoch-{epoch2+1}\" if epoch2 is not None else model2\n",
    "            print(f\"\\\\begin{{filecontents*}}{{{ALL_MODEL_DICT[model2]}{epoch}}}\")\n",
    "            print(cut(responses[model_name][max_i]['responses'][max_j]))\n",
    "            print(\"\\end{filecontents*}\")\n",
    "            print(\"\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\\\begin{{filecontents*}}{{{MAIN_SHORT}{epoch}}}\")\n",
    "        print(cut(responses[main_model][max_i]['responses'][max_j]))\n",
    "        print(\"\\end{filecontents*}\")\n",
    "        print(\"\")\n",
    "\n",
    "        used_data.append(max_i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
